{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we want to build a L-layer (input layer, L-1 hidden layers and output layer, it should be noted that in some descriptions input layer might not be counted in L) neural network for two-classification problem.\n",
    "\n",
    "For activation function in each layer (except the output layer), we use Relu. For output layer, the activation function is sigmoid.\n",
    "\n",
    "The whole model structure is INPUT->[LINEAR -> RELU] × (L-1) -> LINEAR -> SIGMOID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.** Given dim of each layer (layer_dims), initialize parameters of each layer (Hint: You can try other parameter initialization methods, such as Xavier initialization,He initialization and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"layer.JPG\" width=700, heigth=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    layer_dims--a list, record dim of each layer\n",
    "    layer_dim=[]\n",
    "    parameters--a dictionary, record the parameter of each layer\n",
    "    \"\"\"\n",
    "    L=len(layer_dims)\n",
    "    parameters={}\n",
    "    for l in range(1,L): # initialize from layer 1 to layer L\n",
    "        parameters['W'+str(l)]= np.random.randn(layer_dims[l],layer_dims[l-1])*0.1\n",
    "        parameters['b'+str(l)]= np.random.randn(layer_dims[l],1)*0.1\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.** Implement the forward propagation of the LINEAR->ACTIVATION layer\n",
    "$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$\n",
    "$$A^{[l]}=activation(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_active_forward(pre_A,W,b,activation):\n",
    "    \"\"\"\n",
    "    pre_A--output from the last layer  // 应该是previous 不是 last\n",
    "    W,b-- parameters of the current layer\n",
    "    activation--type of activation function\n",
    "    return:\n",
    "    A--output of the activation function\n",
    "    cache--record the linear_cache and activation_cache\n",
    "    \"\"\"\n",
    "    ###calculate the linear output\n",
    "    Z= W @ pre_A + b # shape(nl,1)\n",
    "    linear_cache=(pre_A,W,b)\n",
    "    ###calculate the output of activation function\n",
    "    if activation=='relu':\n",
    "        A= relu(Z)\n",
    "        activation_cache=Z\n",
    "    if activation=='sigmoid':\n",
    "        A= sigmoid(Z)\n",
    "        activation_cache=Z\n",
    "    \n",
    "    cache=(linear_cache,activation_cache)\n",
    "    \n",
    "    return A,cache \n",
    "        \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def sigmoid(x):\n",
    "    return np.exp(x) / (1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.** Implement the forward propogation of the whole model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X,parameters):\n",
    "    \"\"\"\n",
    "    X--data\n",
    "    parameters--parameter of each layer\n",
    "    return:\n",
    "    AL-- output of last layer(activation output)\n",
    "    caches--list, record the cache from each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    caches=[]\n",
    "    A=X # A0\n",
    "    L=len(parameters)//2\n",
    "    for l in range(1, L):\n",
    "        pre_A = A\n",
    "        ### Linear->Relu forward\n",
    "        A, cache =  Linear_active_forward(pre_A,parameters['W'+str(l)],parameters['b'+str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    ### Linear->Sigmoid forward\n",
    "    AL, cache =  Linear_active_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL,caches \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1,'2':2}\n",
    "len(a)\n",
    "L=2\n",
    "str(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4** Compute the logistic cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(AL,Y):\n",
    "    \"\"\"\n",
    "    AL--the predicted probability\n",
    "    Y--true lable\n",
    "    return:\n",
    "    cost--a number\n",
    "    \"\"\"\n",
    "    AL=AL.reshape(Y.shape)\n",
    "    a = Y.shape[0]\n",
    "    b = Y.shape[1]\n",
    "    if a > b:\n",
    "        m = a\n",
    "    else:\n",
    "        m = b\n",
    "    cost = (-1/m)*np.sum(Y * np.log(AL+1e-5) + (1 - Y) * (np.log(1-AL+1e-5))) # 添加正则化，防止溢出\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5** Implement linear backward.\n",
    "<br />\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "<br />\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "<br />\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n",
    "**use the above 3 formulations to implement linear_backward()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \n",
    "    Returns:\n",
    "    dA_pre -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l),same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    pre_A, W, b = cache\n",
    " \n",
    "    m = dZ.shape[1]\n",
    "    #print(m)\n",
    "    dW = (1/m) * (dZ @ pre_A.T)\n",
    "    db = (1/m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "    #dA_pre = W.T @ dZ\n",
    "    dA_pre = np.dot(W.T,dZ)\n",
    "    ###calculate dW,db,dA_pre\n",
    "\n",
    "    \n",
    "    return dA_pre,dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6** \n",
    "1. Implements the backward propagation for Sigmoid and Relu unit.\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "\n",
    "2. Implement the backpropagation for the LINEAR->ACTIVATION layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34678217837590053"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-sigmoid([1,2,3]))@sigmoid([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA,activation_cache):\n",
    "    '''\n",
    "    dA--Gradient of the cost with respect to the activation (of the current layer l)\n",
    "    activation_cache--linear output of the current layer l\n",
    "    return:\n",
    "    dZ--Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    '''\n",
    "    ###compute gradient dZ \n",
    "    dZ = dA * (sigmoid(activation_cache)*(1-sigmoid(activation_cache)))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA,activation_cache):\n",
    "    '''\n",
    "    dA--Gradient of the cost with respect to the activation (of the current layer l)\n",
    "    activation_cache--linear output of the current layer l\n",
    "    return:\n",
    "    dZ--Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    '''\n",
    "    ###compute gradient dZ\n",
    "    v = np.ones(activation_cache.shape)\n",
    "    v[activation_cache <= 0] = 0\n",
    "    dZ = dA * v\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([1,-2,3])\n",
    "v = t.copy()\n",
    "v[t <= 0] = 0\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_active_backward(dA,cache,activation):\n",
    "    \"\"\"\n",
    "    dA--Gradient of the cost with respect to the activation (of the current layer l)\n",
    "    cache--tuple of values (linear_cache, activation_cache)\n",
    "    Returns:\n",
    "    dA_pre -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l),same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache,activation_cache=cache\n",
    "    if activation=='relu':\n",
    "        ###calculate dZ,dA_pre, dW, db\n",
    "        dA_pre,dW,db=linear_backward(relu_backward(dA,activation_cache),linear_cache)\n",
    "    if activation=='sigmoid':\n",
    "        ###calculate dZ,dA_pre, dW, db\n",
    "        dA_pre,dW,db=linear_backward(sigmoid_backward(dA,activation_cache),linear_cache)\n",
    "        \n",
    "    return dA_pre, dW, db\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7.** Implement the backward function for the whole network.\n",
    "<br />\n",
    "To backpropagate through this network, we know that the output is $A^{[L]} = Sigmoid(Z^{[L]})$. we give the code to compute  $dA^{[L]}= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    AL -- probability vector, output of the forward propagation (forward())\n",
    "    Y -- true label\n",
    "    caches -- list of caches containing: every cache of linear_activation_forward() with \"relu\" and the cache of linear_activation_forward() with \"sigmoid\"\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "    grads[\"dA\" + str(l)] = ...\n",
    "    grads[\"dW\" + str(l)] = ...\n",
    "    grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL= - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ###compute the parameter gradients of the output layer (Linear->sigmoid)\n",
    "    current_cache=caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)],grads[\"dW\" + str(L)],grads[\"db\" + str(L)] = linear_active_backward(dAL,current_cache,'sigmoid')\n",
    "    grads[\"dA\" + str(L)] = dAL\n",
    "    ### calculate the parameter gradients(dA,dW,db) of the remaining layers(linear->relu)\n",
    "    for l in reversed(range(1,L)):\n",
    "        ###your code\n",
    "        current_cache=caches[l-1]\n",
    "        grads[\"dA\" + str(l-1)],grads[\"dW\" + str(l)],grads[\"db\" + str(l)] = linear_active_backward(grads[\"dA\" + str(l)],current_cache,'relu')\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dW1'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = reversed(range(1,3))\n",
    "for i in t:\n",
    "    print(i)\n",
    "\"dW\"+str(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7** Implement “update_parameters()” to update all parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    for i in range(1,L+1):\n",
    "        parameters['W'+str(i)] = parameters['W'+str(i)] - grads['dW'+str(i)]*learning_rate\n",
    "        parameters['b'+str(i)] = parameters['b'+str(i)] - grads['db'+str(i)]*learning_rate\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8** Implement the predict() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    "    \"\"\"\n",
    "    X--data\n",
    "    parameters: updated parameters\n",
    "    return:\n",
    "    pred: a binary vector [1,X.shape[1]]\n",
    "    \"\"\"\n",
    "    AL,caches=forward(X,parameters)\n",
    "    pred = np.zeros(AL.shape)\n",
    "    pred[AL>=0.5] = 1\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 9** merge above functions into one, implement model() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,layer_dims,learning_rate,num_iterations):\n",
    "    np.random.seed(1)\n",
    "    costs = [] \n",
    "    ### Parameters initialization.\n",
    "    ###your code\n",
    "    parameters=initialize_parameters(layer_dims)\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        ### Forward propagation: INPUT->[LINEAR -> RELU]*(L-1) -> LINEAR ->SIGMOID.\n",
    "        ### your code\n",
    "        AL,caches=forward(X,parameters)\n",
    "    \n",
    "        ### Compute cost.\n",
    "        current_cost=cost(AL,Y)\n",
    "        #print(current_cost)\n",
    "        costs.append(current_cost)\n",
    "        ### Backward propagation.\n",
    "        ###your code\n",
    "        grads = backward(AL, Y, caches)\n",
    "        ### Update parameters.\n",
    "        ### your code\n",
    "        parameters=update_parameters(parameters,grads,learning_rate)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, current_cost))\n",
    "        \n",
    "    return parameters,costs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 10** Use a picture dataset to train the whole model. Here we give the preprocessing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##preproccessing code\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "np.random.seed(1)\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 209)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 209)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.683452\n",
      "Cost after iteration 50: 0.669858\n",
      "Cost after iteration 100: 0.656306\n",
      "Cost after iteration 150: 0.650846\n",
      "Cost after iteration 200: 0.648524\n",
      "Cost after iteration 250: 0.646813\n",
      "Cost after iteration 300: 0.645145\n",
      "Cost after iteration 350: 0.643477\n",
      "Cost after iteration 400: 0.641687\n",
      "Cost after iteration 450: 0.639730\n",
      "Cost after iteration 500: 0.637615\n",
      "Cost after iteration 550: 0.635257\n",
      "Cost after iteration 600: 0.632687\n",
      "Cost after iteration 650: 0.629264\n",
      "Cost after iteration 700: 0.625064\n",
      "Cost after iteration 750: 0.619920\n",
      "Cost after iteration 800: 0.613311\n",
      "Cost after iteration 850: 0.604560\n",
      "Cost after iteration 900: 0.594599\n",
      "Cost after iteration 950: 0.582802\n",
      "0.6555023923444976\n",
      "0.34\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_dims = [12288, 20, 7, 5, 1]####you can change the dim of hidden layer, or the number of hidden layers\n",
    "learning_rate=0.01\n",
    "num_iterations=1000\n",
    "parameters,costs=model(train_x,train_y,layer_dims,learning_rate,num_iterations)\n",
    "pred_y = predict(train_x,parameters)\n",
    "pred=predict(test_x,parameters)\n",
    "###calculate accuracy\n",
    "accuracy_tr=np.mean(pred_y==train_y)\n",
    "accuracy_te=np.mean(pred==test_y)\n",
    "print(accuracy_tr)\n",
    "print(accuracy_te)\n",
    "###plot the cost\n",
    "###your code\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
